{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas\n",
    "# pip install beatifulsoup\n",
    "# \n",
    "# !pip install pandas\n",
    "!pip install beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |   London Heathrow to Male In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Verified |  Very good flight following an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not Verified |  An hour's delay due to late ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |   I booked through BA becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |   British airways lost bags ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified |   London Heathrow to Male In...\n",
       "1  Not Verified |  Very good flight following an ...\n",
       "2  Not Verified |  An hour's delay due to late ar...\n",
       "3  ✅ Trip Verified |   I booked through BA becaus...\n",
       "4  ✅ Trip Verified |   British airways lost bags ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |   London Heathrow to Male In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Verified |  Very good flight following an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not Verified |  An hour's delay due to late ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |   I booked through BA becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |   British airways lost bags ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified |   London Heathrow to Male In...\n",
       "1  Not Verified |  Very good flight following an ...\n",
       "2  Not Verified |  An hour's delay due to late ar...\n",
       "3  ✅ Trip Verified |   I booked through BA becaus...\n",
       "4  ✅ Trip Verified |   British airways lost bags ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      |   London Heathrow to Male In new business cl...\n",
      "1      |  Very good flight following an equally good ...\n",
      "2      |  An hour's delay due to late arrival of the ...\n",
      "3      |   I booked through BA because Loganair don’t...\n",
      "4      |   British airways lost bags in LHR then foun...\n",
      "                             ...                        \n",
      "995    |  London to Shanghai. The Concorde room in He...\n",
      "996    |  I have often flown British Airways and have...\n",
      "997    |  Good morning. I would like to write a revie...\n",
      "998    | My flight was cancelled 3 days in a row. Was...\n",
      "999    |  Hong Kong to Copenhagen via London. The who...\n",
      "Name: Cleaned_Review, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "df = pd.DataFrame(reviews, columns=['Review'])\n",
    "\n",
    "# Define a function to clean each review\n",
    "def clean_review(text):\n",
    "    # Remove \"✅ Trip Verified\", \"Not Verified\" or any other unnecessary prefix\n",
    "    text = re.sub(r\"✅ Trip Verified|Not Verified|Verified|Unverified|\\\\n\", \"\", text)\n",
    "    # Additional cleaning can be done here, such as removing extra spaces, unwanted characters, etc.\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the cleaning function to the 'Review' column\n",
    "df['Cleaned_Review'] = df['Review'].apply(clean_review)\n",
    "\n",
    "# Print the cleaned reviews\n",
    "print(df['Cleaned_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "7"
      ],
      "text/latex": [
       "7"
      ],
      "text/markdown": [
       "7"
      ],
      "text/plain": [
       "[1] 7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (2.2.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.7/1.5 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 8.0 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp310-cp310-win_amd64.whl (299 kB)\n",
      "     ---------------------------------------- 0.0/299.8 kB ? eta -:--:--\n",
      "     ----------------------------- ------- 235.5/299.8 kB 14.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 299.8/299.8 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 98.2/98.2 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "     ---------------------------------------- 0.0/274.0 kB ? eta -:--:--\n",
      "     -------------------------------- ------- 225.3/274.0 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 235.5/274.0 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 274.0/274.0 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "     ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "     ----------------------------- -------- 235.5/301.8 kB 7.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 301.8/301.8 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.8-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.5/2.2 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.7/2.2 MB 7.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.1/2.2 MB 8.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.3/2.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.4/2.2 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.6/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.7/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.9/2.2 MB 5.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.0/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.6 MB 2.0 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.2/2.6 MB 2.3 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.5/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.6/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.9/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 1.0/2.6 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.1/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.2/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.3/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.5/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.6/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.7/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.8/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.0/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.1/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.2/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.3/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 2.4/2.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.5/2.6 MB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.5/2.6 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.6/2.6 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manas\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, click, nltk, matplotlib, wordcloud\n",
      "Successfully installed click-8.1.8 contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.8 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 nltk-3.9.1 pillow-11.1.0 pyparsing-3.2.1 regex-2024.11.6 tqdm-4.67.1 wordcloud-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk matplotlib wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Manas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Manas\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')  # If you plan to use sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      london heathrow male new business class ba con...\n",
      "1      good flight following equally good flight rome...\n",
      "2      hour delay due late arrival incoming aircraft ...\n",
      "3      booked ba loganair dont representative manches...\n",
      "4      british airway lost bag lhr found sent cologne...\n",
      "                             ...                        \n",
      "995    london shanghai concorde room heathrow termina...\n",
      "996    often flown british airway considered good air...\n",
      "997    good morning would like write review british a...\n",
      "998    flight cancelled day row flying thursday final...\n",
      "999    hong kong copenhagen via london whole experien...\n",
      "Name: Processed_Review, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clean_review(text):\n",
    "    # Remove '✅ Trip Verified' or any similar labels\n",
    "    text = re.sub(r\"✅ Trip Verified|Not Verified|Verified|Unverified|\\n\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and extra spaces\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the cleaning function\n",
    "df['Cleaned_Review'] = df['Review'].apply(clean_review)\n",
    "\n",
    "# Tokenization and removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_clean(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply tokenization and stopword removal\n",
    "df['Processed_Review'] = df['Cleaned_Review'].apply(tokenize_and_clean)\n",
    "\n",
    "# Print cleaned reviews\n",
    "print(df['Processed_Review'])\n",
    "\n",
    "# Word Frequency Analysis\n",
    "all_reviews = ' '.join(df['Processed_Review'])\n",
    "word_tokens = word_tokenize(all_reviews)\n",
    "\n",
    "# Get word frequency\n",
    "# from collections import Counter\n",
    "# word_freq = Counter(word_tokens)\n",
    "\n",
    "# # Print most common words\n",
    "# print(word_freq.most_common(10))\n",
    "\n",
    "# # Visualization with Wordcloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# # Plot the word cloud\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# # Optional: Sentiment Analysis\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# sia = SentimentIntensityAnalyzer()\n",
    "# df['Sentiment'] = df['Processed_Review'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# # Print sentiment analysis results\n",
    "# print(df[['Review', 'Sentiment']].head())\n",
    "\n",
    "# # Example: Positive/Negative Distribution\n",
    "# df['Sentiment_Label'] = df['Sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))\n",
    "\n",
    "# # Plot sentiment distribution\n",
    "# df['Sentiment_Label'].value_counts().plot(kind='bar', color=['green', 'red', 'gray'])\n",
    "# plt.title('Sentiment Distribution')\n",
    "# plt.ylabel('Count')\n",
    "# plt.xlabel('Sentiment')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
